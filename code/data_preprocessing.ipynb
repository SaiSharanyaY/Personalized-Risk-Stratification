{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary Libraries\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5800eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3b/7y4fc4b11yj200gbh1msnq2h0000gn/T/ipykernel_43101/996012386.py:21: DtypeWarning: Columns (7,9,10,12,15,16,23,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_adrdata = pd.read_csv(file_adrdata, encoding='ISO-8859-1')\n",
      "/var/folders/3b/7y4fc4b11yj200gbh1msnq2h0000gn/T/ipykernel_43101/996012386.py:21: DtypeWarning: Columns (7,12,15,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_adrdata = pd.read_csv(file_adrdata, encoding='ISO-8859-1')\n",
      "/var/folders/3b/7y4fc4b11yj200gbh1msnq2h0000gn/T/ipykernel_43101/996012386.py:21: DtypeWarning: Columns (7,12,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_adrdata = pd.read_csv(file_adrdata, encoding='ISO-8859-1')\n"
     ]
    }
   ],
   "source": [
    "# Data is organised into a single csv for the chosen time frame 2020 december - 2024 may\n",
    "\n",
    "years = [2020,2021,2022,2023,2024]\n",
    "\n",
    "# Define the raw data file path format for all three - VAERSSYMPTOMS.csv, VAERSVAX.csv, VAERSDATA.csv\n",
    "file_pattern_symptoms = '/Users/sharanya/Documents/Personalized-Risk-Stratification/Data/Rawdata/{year}VAERSSYMPTOMS.csv'\n",
    "file_pattern_vax = '/Users/sharanya/Documents/Personalized-Risk-Stratification/Data/Rawdata/{year}VAERSVAX.csv'\n",
    "file_pattern_demographics = '/Users/sharanya/Documents/Personalized-Risk-Stratification/Data/Rawdata/{year}VAERSDATA.csv' \n",
    "\n",
    "# Initialize an empty list to hold dataframes\n",
    "dfs = []\n",
    "\n",
    "for year in years:\n",
    "    # Construct file paths for the year\n",
    "    file_symptoms = file_pattern_symptoms.format(year=year)\n",
    "    file_vax = file_pattern_vax.format(year=year)\n",
    "    file_demographics = file_pattern_demographics.format(year=year)\n",
    "    \n",
    "    # Read the files into DataFrames\n",
    "    df_symptoms = pd.read_csv(file_symptoms, encoding='ISO-8859-1')\n",
    "    df_vax = pd.read_csv(file_vax, encoding='ISO-8859-1')\n",
    "    df_demographics = pd.read_csv(file_demographics, encoding='ISO-8859-1') \n",
    "    \n",
    "    # Filter the vax DataFrame to only include 'COVID19' and 'COVID19-2' in 'vax_type'\n",
    "    df_vax_filtered = df_vax[df_vax['VAX_TYPE'].isin(['COVID19', 'COVID19-2'])]\n",
    "    \n",
    "    # Merge the filtered vax DataFrame with symptoms and third file DataFrames\n",
    "    merged_df = pd.merge(df_symptoms, df_vax_filtered, on='VAERS_ID')\n",
    "    merged_df = pd.merge(merged_df, df_demographics, on='VAERS_ID')\n",
    "    \n",
    "    # Append the merged DataFrame to the list\n",
    "    dfs.append(merged_df)\n",
    "\n",
    "# Concatenate all yearly DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional - save the .csv file before proceeding to the next step (uncomment and run below commented line)\n",
    "# combined_df.to_csv('/Data/Combined_VAERS_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d817871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only relevant features\n",
    "data = combined_df[['VAERS_ID', 'SYMPTOM1', 'SYMPTOM2', 'SYMPTOM3', 'SYMPTOM4', 'SYMPTOM5', 'VAX_DOSE_SERIES', 'VAX_NAME', 'VAX_ROUTE', 'STATE', 'AGE_YRS', 'SEX', 'OTHER_MEDS', 'CUR_ILL', 'HISTORY', 'PRIOR_VAX', 'ALLERGIES']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef775b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering only patient records with dose series 1 or 2 for better analysis on ADR\n",
    "data = data[data['VAX_DOSE_SERIES'].isin(['1', '2'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed4fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age column preprocessing, converting to categorical for better analysis\n",
    "def convertAge(string_age):\n",
    "    if 0 <= float(string_age) <= 2: # 0 - 3 yrs\n",
    "        return \"Infant\"\n",
    "    if 3 <= float(string_age) <= 12: # 4 - 15 yrs\n",
    "        return \"Kid\"\n",
    "    if 13 <= float(string_age) <= 19: # 13 - 19 yrs\n",
    "        return \"Teenager\"\n",
    "    if 20 <= float(string_age) <= 30: # 20 - 29 yrs\n",
    "        return \"Young Adult\"\n",
    "    if 31 <= float(string_age) <= 59: # 30 - 59 yrs\n",
    "        return \"Adult\"\n",
    "    return \"Senior Citizen\" # >60 yrs\n",
    "\n",
    "data['AGE_YRS'] = data['AGE_YRS'].apply(lambda x: convertAge(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3ba7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column preprocessing - handling missing values and data imputation\n",
    "\n",
    "data[\"VAX_ROUTE\"] = data['VAX_ROUTE'].fillna(\"UNK\")\n",
    "\n",
    "# Function to clean state abbreviations\n",
    "def clean_state(state):  \n",
    "    if state in official_states:\n",
    "        return state\n",
    "    elif state == \"TX\" or state == \"CA\":\n",
    "        return state  \n",
    "    elif state == \"Tx\":  \n",
    "        return \"TX\"\n",
    "    elif state == \"ca\":\n",
    "        return \"CA\"\n",
    "    else:\n",
    "        return \"UNK\"  # Replace unrecognized states with \"UNK\"\n",
    "\n",
    "# List of official USPS two-letter state abbreviations\n",
    "official_states = {\n",
    "    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID',\n",
    "    'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS',\n",
    "    'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK',\n",
    "    'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV',\n",
    "    'WI', 'WY', 'DC', 'GU', 'PR', 'VI'\n",
    "}\n",
    "# Apply the function to the dataset\n",
    "data['STATE'] = data['STATE'].apply(clean_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#consider splitting the data by vaccine brands - pfizer, moderna\n",
    "\n",
    "data_pfizer = data[data['VAX_NAME'].isin(['COVID19 (COVID19 (PFIZER-BIONTECH))', 'COVID19 (COVID19 (PFIZER-BIONTECH BIVALENT))'])]\n",
    "data_moderna = data[data['VAX_NAME'].isin(['COVID19 (COVID19 (MODERNA))', 'COVID19 (COVID19 (MODERNA BIVALENT))'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to consider only the rows with atlease one labelled adr present - to retain records with relevant adverse reactions only.\n",
    "def filter_by_symptoms(dataframe, reaction_list, symptom_columns):\n",
    "    \"\"\"\n",
    "    Filters rows in the DataFrame based on the presence of labelled adr in the specified symptom columns.\n",
    "    Args:\n",
    "        dataframe: The DataFrame to filter.\n",
    "        reaction_list: A list of adr to filter by.\n",
    "        symptom_columns: The list of column names to check for reactions.\n",
    "    Returns:\n",
    "        pd.DataFrame: A filtered DataFrame containing only the rows where at least one of the reactions is found in the specified columns.\n",
    "    \"\"\"\n",
    "    symptom_data = dataframe[symptom_columns].apply(lambda col: col.str.lower())\n",
    "    mask = symptom_data.isin(reaction_list).any(axis=1)\n",
    "    return dataframe[mask]\n",
    "\n",
    "# list of all the adr as per their company labellings.\n",
    "pfizer_adr = ['chest pain', 'shortness of breath', 'breath shortness', 'difficulty breathing', 'pounding heartbeat', 'fast heartbeat', 'rash', 'itch', 'hives', 'face swelling', 'myocarditis', 'pericarditis', 'injection site pain', 'injection site reaction', 'injection site swelling', 'injection site redness', 'tierdness', 'headache', 'muscle pain', 'chills', 'joint pain', 'fever', 'nausea', 'feeling unwell', 'uneasy feeling', 'swollen lymph nodes', 'decreased appetite','diarrhea', 'vomiting', 'arm pain', 'fainting', 'dizziness']\n",
    "moderna_adr = ['myocarditis','pericarditis','cardiac','anaphylaxis','urticaria','syncope','pain','auxillary swelling','auxillary tenderness','swelling','redness','fatigue','headache','myalgia','arthralgia','chills','fever','nausea','lymphadenopathy','erythemea','injection site reaction','injection site redness','injection site swelling','injection site itch','injection site rash']\n",
    "symptom_columns = [\"SYMPTOM1\", \"SYMPTOM2\", \"SYMPTOM3\", \"SYMPTOM4\", \"SYMPTOM5\"]\n",
    "\n",
    "pfizer_filtered_by_symptoms = filter_by_symptoms(data_pfizer, pfizer_adr, symptom_columns)\n",
    "moderna_filtered_by_symptoms = filter_by_symptoms(data_moderna, moderna_adr, symptom_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a661739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for extracting all the unique symptoms from the 5 symptom columns \n",
    "def get_unique_values(df, column_prefix, start, end):\n",
    "    \"\"\"\n",
    "    Extract unique values from specified columns in a DataFrame.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        column_prefix (str): The prefix of the column names (e.g., 'SYMPTOM').\n",
    "        start (int): The starting index for the column range.\n",
    "        end (int): The ending index for the column range.\n",
    "    Returns:\n",
    "        list: A combined list of unique values from the specified columns.\n",
    "    \"\"\"\n",
    "    all_unique_values = set()\n",
    "    for i in range(start, end + 1):\n",
    "        col_name = f\"{column_prefix}{i}\"\n",
    "        if col_name in df.columns:  \n",
    "            column_unique = df[col_name].unique()\n",
    "            all_unique_values.update(column_unique)\n",
    "    return list(all_unique_values)\n",
    "\n",
    "# Call the function\n",
    "pfizer_symptom_list = get_unique_values(pfizer_filtered_by_symptoms, column_prefix=\"SYMPTOM\", start=1, end=5)\n",
    "moderna_symptom_list = get_unique_values(moderna_filtered_by_symptoms, column_prefix=\"SYMPTOM\", start=1, end=5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8a986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Disease Extraction using BioBERT to retain only the relevant health condition terms in the columns - PastHC and CUR_ILL\n",
    "# This is a pretrained model for Named Entity Recognition (NER) in biomedical texts, specifically for disease-related terms.\n",
    "# This is mainly used to get rid of the irrelevant terms in the columns.\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"alvaroalon2/biobert_diseases_ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create a NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Function to clean and reconstruct entities\n",
    "def reconstruct_entities(entities):\n",
    "    \"\"\"\n",
    "    Reconstructs named entities by combining subword tokens and removing artifacts.\n",
    "    \n",
    "    Args: \n",
    "        entities (list): A list of dictionaries containing NER results, each with 'word' and optionally 'entity_group'.\n",
    "    \n",
    "    Return: \n",
    "        list: A list of reconstructed entity strings with subwords merged.\n",
    "    \"\"\"\n",
    "    reconstructed_terms = []\n",
    "    for entity in entities:\n",
    "        word = entity.get('word', '')\n",
    "        if word.startswith(\"##\"):\n",
    "            if reconstructed_terms:\n",
    "                reconstructed_terms[-1] += word[2:]  # Merge subword with the previous word\n",
    "            else:\n",
    "                reconstructed_terms.append(word[2:])\n",
    "        else:\n",
    "            reconstructed_terms.append(word)\n",
    "    return reconstructed_terms\n",
    "\n",
    "# Function to chunk long text\n",
    "def chunk_text(text, max_length=128):\n",
    "    \"\"\"\n",
    "    Splits input text into smaller chunks based on token length.\n",
    "    \n",
    "    Args: \n",
    "        text (str): The input text that's to be chunked.\n",
    "        max_length (int): The maximum number of tokens per chunk (default is 128).\n",
    "    \n",
    "    Return: \n",
    "        list: A list of decoded text chunks, each within the token limit.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0].tolist()\n",
    "    chunks = [tokens[i : i + max_length] for i in range(0, len(tokens), max_length)]\n",
    "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "# Function to extract disease-related terms\n",
    "def extract_diseases(text):\n",
    "    \"\"\"\n",
    "    Extracts disease related terms from input biomedical text using a BioBERT-based NER pipeline.\n",
    "    \n",
    "    Args: \n",
    "        text (str): The biomedical input text to extract disease entities from.\n",
    "    \n",
    "    Return: \n",
    "        list: A list of unique, reconstructed disease-related terms extracted from the text.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Handle non-string or empty inputs\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return []  \n",
    "\n",
    "        # Split long text into chunks\n",
    "        text_chunks = chunk_text(text, max_length=512)\n",
    "\n",
    "        # Apply the NER pipeline to each chunk\n",
    "        all_entities = []\n",
    "        for chunk in text_chunks:\n",
    "            entities = ner_pipeline(chunk)\n",
    "            # Debugging: Print raw entities\n",
    "            print(f\"Raw entities for text: {chunk[:50]}... {entities}\")\n",
    "            # Skip if entities are not a list\n",
    "            if not isinstance(entities, list):\n",
    "                continue  \n",
    "            all_entities.extend(entities)\n",
    "\n",
    "        # Filter entities for disease-related terms and reconstruct them\n",
    "        disease_terms = [entity for entity in all_entities if 'entity_group' in entity and entity['entity_group'] in ['DISEASE']]\n",
    "        reconstructed_terms = reconstruct_entities(disease_terms)\n",
    "        return list(set(reconstructed_terms))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}. Error: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce15ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moderna data disease term extraction\n",
    "moderna_filtered_by_symptoms.loc[:, 'ExtractedPastHC'] = moderna_filtered_by_symptoms['HISTORY'].apply(extract_diseases)\n",
    "moderna_filtered_by_symptoms.loc[:, 'ExtractedCurrentHC'] = moderna_filtered_by_symptoms['CUR_ILL'].apply(extract_diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfizer data disease term extraction\n",
    "pfizer_filtered_by_symptoms.loc[:, 'ExtractedPastHC'] = pfizer_filtered_by_symptoms['HISTORY'].apply(extract_diseases)\n",
    "pfizer_filtered_by_symptoms.loc[:, 'ExtractedCurrentHC'] = pfizer_filtered_by_symptoms['CUR_ILL'].apply(extract_diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e979afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"judithrosell/BC5CDR_ClinicalBERT_NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create a NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "\n",
    "# Function to extract disease-related terms\n",
    "def extract_medications(text):\n",
    "    \"\"\"\n",
    "    Extracts medication related terms from clinical text using a ClinicalBERT-based NER pipeline.\n",
    "   \n",
    "    Args:\n",
    "        text (str): Clinical input text to analyze for chemical or medication entities.\n",
    "    \n",
    "    Return:\n",
    "        list: A list of unique medication terms found in the input text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return []  # Handle non-string or empty inputs\n",
    "\n",
    "        # Split long text into chunks\n",
    "        text_chunks = chunk_text(text, max_length=512)\n",
    "\n",
    "        # Apply the NER pipeline to each chunk\n",
    "        all_entities = []\n",
    "        for chunk in text_chunks:\n",
    "            entities = ner_pipeline(chunk)\n",
    "\n",
    "            # Debugging: Print raw entities\n",
    "            print(f\"Raw entities for text: {chunk[:50]}... {entities}\")\n",
    "\n",
    "            if not isinstance(entities, list):\n",
    "                continue  # Skip if entities are not a list\n",
    "\n",
    "            all_entities.extend(entities)\n",
    "\n",
    "        # Filter entities for disease-related terms and reconstruct them\n",
    "        medicine_terms = [entity for entity in all_entities if 'entity_group' in entity and entity['entity_group'] in ['Chemical']]\n",
    "        reconstructed_terms = reconstruct_entities(medicine_terms)\n",
    "        return list(set(reconstructed_terms))  # Return unique disease terms\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}. Error: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a4458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# medical term extraction from moderna and pfizer dataframes.\n",
    "pfizer_filtered_by_symptoms.loc[:, 'ExtractedMedications'] = pfizer_filtered_by_symptoms['OTHER_MEDS'].apply(extract_medications)\n",
    "moderna_filtered_by_symptoms.loc[:, 'ExtractedMedications'] = moderna_filtered_by_symptoms['OTHER_MEDS'].apply(extract_medications)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cebb6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicates based on VAERS_ID\n",
    "pfizer_sample = pfizer_filtered_by_symptoms.drop_duplicates(subset=[\"VAERS_ID\"])\n",
    "moderna_sample = moderna_filtered_by_symptoms.drop_duplicates(subset=[\"VAERS_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d4db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function froms an excel of all the retained symptoms along with their frequencies\n",
    "# these are used later to perform manual labelling and validation of the symptoms\n",
    "\n",
    "def get_symptom_frequencies(dataframe, columns):\n",
    "    \"\"\"\n",
    "    Extract unique symptom terms from multiple columns and compute their frequencies.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The DataFrame containing symptom columns.\n",
    "        columns (list): List of column names to extract and process.\n",
    "    \n",
    "    Return:\n",
    "        pd.DataFrame: A DataFrame with 'Symptom' and 'Count' columns, sorted by count.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_text = ''\n",
    "    for col in columns:\n",
    "        all_text += ' ' + ' '.join(dataframe[col].dropna().astype(str))\n",
    "    \n",
    "    words = re.findall(r'\\b[\\w\\s]+\\b', all_text.lower())\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    symptom_freq_df = pd.DataFrame(word_counts.items(), columns=['Symptom', 'Count'])\n",
    "    symptom_freq_df['Symptom'] = symptom_freq_df['Symptom'].str.title()\n",
    "    symptom_freq_df = symptom_freq_df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return symptom_freq_df\n",
    "\n",
    "\n",
    "moderna_symptom_freq = get_symptom_frequencies(moderna_sample, symptom_columns)\n",
    "moderna_symptom_freq.to_excel('/Users/sharanya/Documents/Personalized-Risk-Stratification/Data/Target Label/moderna_symptom_data.xlsx', index=False)\n",
    "pfizer_symptom_freq = get_symptom_frequencies(pfizer_sample, symptom_columns)\n",
    "pfizer_symptom_freq.to_excel('/Users/sharanya/Documents/Personalized-Risk-Stratification/Data/Target Label/pfizer_symptom_data.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7882d49b",
   "metadata": {},
   "source": [
    "### These symptom data Excel files were used for manual review. We carefully examined all listed symptoms and selected only the relevant adverse drug reaction terms, categorizing each under its appropriate organ-system classification.\n",
    "\n",
    "### While this process may seem repetitive, there’s an important distinction: \n",
    "\n",
    "### we reviewed all symptoms reported by patients who had at least one relevant ADR. This approach allowed us to identify additional relevant ADRs that might have been missed initially from company labellings. As a result, we were able to develop a thoroughly refined and accurately labeled dataset, verified through manual review and cross-checked by a medical professional.\n",
    "\n",
    "### the manually edited excel files are attached in this repo, so they could be used to run further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c773e7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3b/7y4fc4b11yj200gbh1msnq2h0000gn/T/ipykernel_43101/3133221682.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  moderna_target_df['TargetLabel'] = moderna_target_df['TargetLabel'].str.split(',')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Output saved to: /Users/sharanya/Documents/Personalized-Risk-Stratification/Data/Target Label /moderna_target_analysis.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Load the manually edited excel files for further steps.\n",
    "file_path = \"/Users/sharanya/Documents/Personalized-Risk-Stratification/Data/Target Label /moderna_symptom_data_edit_2.xls\" \n",
    "moderna_target = pd.read_excel(file_path)  \n",
    "\n",
    "moderna_target_df = moderna_target[['Selected_ADR', 'Frequency', 'TargetLabel']]\n",
    "\n",
    "# Split multiple target labels into a list\n",
    "moderna_target_df['TargetLabel'] = moderna_target_df['TargetLabel'].str.split(',')\n",
    "\n",
    "# Explode so, each row has only one target label\n",
    "moderna_df_exploded = moderna_target_df.explode('TargetLabel')\n",
    "\n",
    "# Remove leading/trailing spaces from TargetLabel names\n",
    "moderna_df_exploded['TargetLabel'] = moderna_df_exploded['TargetLabel'].str.strip()\n",
    "\n",
    "# Group by TargetLabel but preserve individual ADRs and frequencies\n",
    "target_label_dict = {}\n",
    "for label in moderna_df_exploded['TargetLabel'].unique():\n",
    "    filtered_df = moderna_df_exploded[moderna_df_exploded['TargetLabel'] == label]\n",
    "    \n",
    "    # Store ADR for each target label\n",
    "    target_label_dict[label] = {\n",
    "        \"ADR\": filtered_df[['Selected_ADR', 'Frequency']].values.tolist()\n",
    "    }\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "moderna_result_df = pd.DataFrame.from_dict(target_label_dict, orient='index')\n",
    "\n",
    "# Reset index to move Target Label into a column\n",
    "moderna_result_df.reset_index(inplace=True)\n",
    "\n",
    "moderna_result_df.columns = [\"Target Label\", \"ADR_Frequency_List\"]\n",
    "\n",
    "# ADR with frequency < 10 are removed, to avoid extreme rare cases.\n",
    "moderna_result_df['ADR_Frequency_List'] = moderna_result_df['ADR_Frequency_List'].apply(\n",
    "    lambda items: [entry for entry in items if entry[1] >= 10]\n",
    ")\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_path = \"/Users/sharanya/Documents/Personalized-Risk-Stratification/Data/Target Label /moderna_target_analysis.xlsx\"\n",
    "moderna_result_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(\"Processing complete. Output saved to:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c93414e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Output saved to: /Users/sharanya/Documents/Personalized-Risk-Stratification/Data/Target Label /pfizer_target_analysis.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3b/7y4fc4b11yj200gbh1msnq2h0000gn/T/ipykernel_43101/2988428967.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pfizer_target_df['TargetLabel'] = pfizer_target_df['TargetLabel'].str.split(',')\n"
     ]
    }
   ],
   "source": [
    "# Load the manually edited excel files for further steps.\n",
    "file_path = \"/Users/sharanya/Documents/Personalized-Risk-Stratification/Data/Target Label /pfizer_symptom_data_edit_2.xls\" \n",
    "pfizer_target = pd.read_excel(file_path)  \n",
    "\n",
    "pfizer_target_df = pfizer_target[['Selected_ADR', 'Frequency', 'TargetLabel']]\n",
    "\n",
    "# Split multiple target labels into a list\n",
    "pfizer_target_df['TargetLabel'] = pfizer_target_df['TargetLabel'].str.split(',')\n",
    "\n",
    "# Explode so, each row has only one target label\n",
    "pfizer_df_exploded = pfizer_target_df.explode('TargetLabel')\n",
    "\n",
    "# Remove leading/trailing spaces from TargetLabel names\n",
    "pfizer_df_exploded['TargetLabel'] = pfizer_df_exploded['TargetLabel'].str.strip()\n",
    "\n",
    "# Group by TargetLabel but preserve individual ADRs and frequencies\n",
    "target_label_dict = {}\n",
    "for label in pfizer_df_exploded['TargetLabel'].unique():\n",
    "    filtered_df = pfizer_df_exploded[pfizer_df_exploded['TargetLabel'] == label]\n",
    "    \n",
    "    # Store ADR for each target label\n",
    "    target_label_dict[label] = {\n",
    "        \"ADR\": filtered_df[['Selected_ADR', 'Frequency']].values.tolist()\n",
    "    }\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "pfizer_result_df = pd.DataFrame.from_dict(target_label_dict, orient='index')\n",
    "\n",
    "# Reset index to move Target Label into a column\n",
    "pfizer_result_df.reset_index(inplace=True)\n",
    "\n",
    "pfizer_result_df.columns = [\"Target Label\", \"ADR_Frequency_List\"]\n",
    "\n",
    "# ADR with frequency < 10 are removed, to avoid extreme rare cases.\n",
    "pfizer_result_df['ADR_Frequency_List'] = pfizer_result_df['ADR_Frequency_List'].apply(\n",
    "    lambda items: [entry for entry in items if entry[1] >= 10]\n",
    ")\n",
    "\n",
    "# Save results to an Excel file\n",
    "output_path = \"/Users/sharanya/Documents/Personalized-Risk-Stratification/Data/Target Label /pfizer_target_analysis.xlsx\"\n",
    "pfizer_result_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(\"Processing complete. Output saved to:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load teh target label data for the final step\n",
    "df_labels = pd.read_excel(\"/Users/sharanya/Documents/Personalized-Risk-Stratification/Data/Target Label /moderna_target_analysis_2.xls\")\n",
    "\n",
    "# Extract list of all target labels, removing any NaN values\n",
    "MODERNA_TargetLabel = df_labels['Target Label'].dropna().tolist()\n",
    "\n",
    "# Dictionary to map each target label to its associated ADR terms\n",
    "label_to_adrs = {}\n",
    "\n",
    "# Iterate through each row of the label DataFrame\n",
    "for idx, row in df_labels.iterrows():\n",
    "    moderna_target_label = row[\"Target Label\"]\n",
    "    adr_freq_list_str = row[\"ADR_Frequency_List\"]\n",
    "\n",
    "    # If ADR list is missing, map label to empty set\n",
    "    if pd.isna(adr_freq_list_str):\n",
    "        label_to_adrs[moderna_target_label] = set()\n",
    "        continue\n",
    "\n",
    "    # Convert ADR frequency string back into a list of tuples\n",
    "    adr_freq_list = ast.literal_eval(adr_freq_list_str)\n",
    "    # Extract and normalize ADR terms to lowercase\n",
    "    adr_terms = set([str(t[0]).strip().lower() for t in adr_freq_list])\n",
    "\n",
    "    # Assign processed ADR terms to the corresponding label\n",
    "    label_to_adrs[moderna_target_label] = adr_terms\n",
    "\n",
    "# Initialize columns with empty strings\n",
    "for t_label in label_to_adrs.keys():\n",
    "    moderna_sample[t_label] = \"\"\n",
    "\n",
    "# Row-wise iteration\n",
    "for idx, row in moderna_sample.iterrows():\n",
    "    row_symptoms = set(\n",
    "        str(row[col]).strip().lower()\n",
    "        for col in symptom_columns\n",
    "        if not pd.isna(row[col])\n",
    "    )\n",
    "\n",
    "    # Compare symptoms to ADRs under each target label, if matching ADRs are found, join and assign to the corresponding target label column\n",
    "    for t_label, adr_set in label_to_adrs.items():\n",
    "        matched_adrs = row_symptoms.intersection(adr_set)\n",
    "        if matched_adrs:\n",
    "            moderna_sample.at[idx, t_label] = \", \".join(matched_adrs)\n",
    "\n",
    "MODERNA_df = moderna_sample.loc[:, moderna_sample.columns.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b83e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retaining only needed columns for the final DataFrame\n",
    "needed_col = ['SEX', 'AGE_YRS', 'VAX_ROUTE', 'VAX_DOSE_SERIES', 'ExtractedPastHC', 'ExtractedMedications', 'Gastrointestinal Issues', 'Pain Syndromes', 'Psychological Disorders',\n",
    "    'Musculoskeletal Disorders', 'Fever', 'Dermatological Conditions',\n",
    "    'Neurological Disorders', 'Swelling', 'Injection Site Reaction']\n",
    "\n",
    "# Update the DataFrame to retain only the specified columns\n",
    "MODERNA_df = MODERNA_df[needed_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00816b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final dataset with labels for further analysis.\n",
    "MODERNA_df.to_csv(\"moderna_with_labels_dose_1+2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_excel(\"Target label formation/pfizer_target_analysis_2.xlsx\")\n",
    "\n",
    "# Extract list of all target labels, removing any NaN values\n",
    "PFIZER_TargetLabel = df_labels['Target Label'].dropna().tolist()\n",
    "\n",
    "# Dictionary to map each target label to its associated ADR terms\n",
    "label_to_adrs = {}\n",
    "\n",
    "# Iterate through each row of the label DataFrame\n",
    "for idx, row in df_labels.iterrows():\n",
    "    pfizer_target_label = row[\"Target Label\"]\n",
    "    adr_freq_list_str = row[\"ADR_Frequency_List\"]\n",
    "\n",
    "    # If ADR list is missing, map label to empty set\n",
    "    if pd.isna(adr_freq_list_str):\n",
    "        label_to_adrs[pfizer_target_label] = set()\n",
    "        continue\n",
    "    \n",
    "    # Convert ADR frequency string back into a list of tuples\n",
    "    adr_freq_list = ast.literal_eval(adr_freq_list_str)\n",
    "    # Extract and normalize ADR terms to lowercase\n",
    "    adr_terms = set([str(t[0]).strip().lower() for t in adr_freq_list])\n",
    "\n",
    "    # Assign processed ADR terms to the corresponding label\n",
    "    label_to_adrs[pfizer_target_label] = adr_terms\n",
    "\n",
    "# Initialize columns with empty strings\n",
    "for t_label in label_to_adrs.keys():\n",
    "    pfizer_sample[t_label] = \"\"\n",
    "\n",
    "# Row-wise iteration\n",
    "for idx, row in pfizer_sample.iterrows():\n",
    "    row_symptoms = set(\n",
    "        str(row[col]).strip().lower()\n",
    "        for col in symptom_columns\n",
    "        if not pd.isna(row[col])\n",
    "    )\n",
    "\n",
    "    # Compare symptoms to ADRs under each target label, if matching ADRs are found, join and assign to the corresponding target label column\n",
    "    for t_label, adr_set in label_to_adrs.items():\n",
    "        matched_adrs = row_symptoms.intersection(adr_set)\n",
    "        if matched_adrs:\n",
    "            pfizer_sample.at[idx, t_label] = \", \".join(matched_adrs)\n",
    "\n",
    "PFIZER_df = pfizer_sample.loc[:, pfizer_sample.columns.notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retaining only needed columns for the final DataFrame\n",
    "needed_col = ['SEX', 'AGE_YRS', 'VAX_ROUTE', 'VAX_DOSE_SERIES', 'ExtractedPastHC', 'ExtractedMedications', 'Gastrointestinal Issues', 'Pain Syndromes', 'Psychological Disorders',\n",
    "    'Musculoskeletal Disorders', 'Fever', 'Dermatological Conditions',\n",
    "    'Neurological Disorders', 'Postural Disorders', 'Cardiovascular Conditions', 'Respiratory Symptoms', 'Injection Site Reaction']\n",
    "\n",
    "# Update the DataFrame to retain only the specified columns\n",
    "PFIZER_df = PFIZER_df[needed_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469d52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final dataset with labels for further analysis.\n",
    "PFIZER_df.to_csv(\"pfizer_with_labels_dose_1+2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
